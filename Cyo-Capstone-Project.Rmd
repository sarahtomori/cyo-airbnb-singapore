---
title: "Cya-harvard-airbnb"
author: "Sarah Tomori"
date: "04/01/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#1. Introduction

This project is part of the Harvard course "Capstone", which is one of 9 required in order to obtain a professional certificate in Data Science. 

In this "Choose Your Own"-project, I have chosen to use the dataset from Kaggle.com, consisting of Airbnb data in Singapore. Airbnb has gained popularity all over the world and is among many travellers a preferred form of accomodation over hotels. Thus, the hotel industry faces a lot of competetion from these private house owners looking to rent out their homes to holiday visiters. 

A various number of things could contribute to determining, how much should be charged for a visitor to stay in an Airbnb. Thus, the purpose of the project is to try to predict, which factors (variable) that affect the price of an Airbnb in Singapore the most. 

 Load all the packages required.

```{r}
library("readr")
library("caret")
library("magrittr")
library("dplyr")
library("wordcloud")
library("SnowballC")
library("corrplot")
library("wesanderson")
library("maps")
library("mapproj")
library("ggmap")
library("textmining")
library("Hmisc")
library("rpart")
library("rsq")
library("glmnet")
library("neuralnet")
```

Read the file with the Singapore Airbnb data.

```{r}
#Read the file with the Singapore Airbnb data. 

#Singapore data set loaded from:
#http://data.insideairbnb.com/singapore/sg/singapore/2019-11-26/visualisations/listings.csv

dl <- tempfile()
download.file("http://data.insideairbnb.com/singapore/sg/singapore/2019-11-26/visualisations/listings.csv", dl)

data <- read_csv(dl)
```

Then you need to create the test set, which will be 10% of the whole data set. The test set will be kept till the very end and will ONLY be used to run the finally chosen algorithm on.

```{r}
# Create test set as 10% of the dataset
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = data$price, times = 1, p = 0.1, list = FALSE)
airbnb_singapore <- data[-test_index,]
temp <- data[test_index,]
```

You need to make sure that the host id that is present in the test set is also present in training set.

```{r}
# Make sure that host_id in the test set are present in the airbnb_singapore set
test_set <- temp %>%
  semi_join(airbnb_singapore, by = "host_id")
# Add rows removed from test set back into airbnb_singapore set
removed <- anti_join(temp, test_set)
airbnb_singapore <- rbind(airbnb_singapore, removed)
rm(temp, data, removed)
```

A validation set is also created and this set is meant to test the intermediary models when training, before deciding on the best and final model.

```{r}
# Split airbnb_singapore data further in to validation and training sets
# Validation set will be 10% of current airbnb_singapore data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index2 <- createDataPartition(y = airbnb_singapore$price, times = 1, p = 0.1, list = FALSE)
train_set <- airbnb_singapore[-test_index2,]
temp2 <- airbnb_singapore[test_index2,]
```

As done with the creation of the test set, we need to make sure that host_id in the validation set is also in the training set.

```{r}
# Make sure host_id in validation is also in training set
validation <- temp2 %>% 
  semi_join(train_set, by = "host_id")
# Add rows removed from validation back into training set
removed <- anti_join(temp2, validation)
train_set <- rbind(train_set, removed)
rm(temp2, removed)
```

## 1.2 Exploring the data

The next step is to explore the data before analysing it. As the data is taken from Kaggle.com and is ready to use, not much data cleaning was necessesary to do beforehand.

```{r}
# Check class of data
class(airbnb_singapore)
```

We can see that our data is a data frame.

```{r}
# Get brief overview of the data
glimpse(airbnb_singapore)
```

From above, it is shown that there are 16 variables in the data set. id is used to identify the Airbnb listing and is unique for each listing. host_id is used to identify the host responsible for the respective listing - it is possible for a host to have more than one listing, if they rent out more than one Airbnb.
 
```{r}
# See a summary of the data
summary(airbnb_singapore)
```

Looking at this summary, we see that the minimum minimum price shown is 0 Singaporean dollars per night while the maximum shown is 10000 Singaporean dollars per night.

```{r}
# Check whether there are any missing values
anyNA(airbnb_singapore)
```

There are some missing values in the dataset. We might have to process that before starting the analysis.

```{r}
# Show unique values of airbnb prices
unique(airbnb_singapore$price)

#Show the number of unique values of prices
n_distinct(unique(airbnb_singapore$price))
```

There is a great range of various unique prices, which is due to the continuous nature of this variable. In fact, there are 364 different prices in the dataset.

Next, we can show how many hosts, listings, regions, neighbourhoods and types of rooms the dataset contains in total.

```{r}
# Show number of hosts, listings, regions, neighbourhoods and room types
airbnb_singapore %>% summarize(n_hosts = n_distinct(host_id), n_listings = n_distinct(id), n_regions = n_distinct(neighbourhood_group), n_neighbourhoods = n_distinct(neighbourhood), n_types = n_distinct(room_type))
```

This shows that there are 2705 hosts, 7305 listings, 5 regions, 43 neighbourhoods and 3 types of rooms possible to rent. The fact that the number of hosts is less than half the number of listings, confirms the earlier statement claiming that some hosts will likely be hosts for more than one listing.

Next, we will look at the minimum nights required in order to stay in an Airbnb in Singapore.
```{r}
# Show average minimum nights required
mean(airbnb_singapore$minimum_nights)
```

The average minimum number of nights required to stay at an Airbnb is 17.59 nights. This indicates that many owners require you to stay for a longer period of time in order to rent out their accommodation. However, it could also be an indication that there are many listings available on the Singaporean Airbnb platform with the purpose of long duration rental.

We can also show what the average price level is per night in Singapore.
```{r}
# Show average price per night
mean(airbnb_singapore$price)
```

The average price is 168 Singaporean dollars.

# Visualisation of the data 

It is necessary to review how our data looks before we proceed to the analysing stage of the project. 

First, a histogram is created to show the distribution of prices among the various listings.
```{r}
# Price distributions in the training set
airbnb_singapore %>% ggplot(aes(price)) + geom_histogram(color = "#000000", fill = "#FFCCCC") +
ggtitle("Distribution of prices across Airbnbs")
```

From the histogram above, we see that majority of the Airbnb listings are in the cheaper end of the scale. The further to the right one looks, the fewer Airbnbs are typically to find at that price level. It also shows that there are many various price levels once the Airbnb costs less than 25 Singaporean dollars per night.

Due to the continuous nature of the prices and the large variation in prices, it is a bit hard to interpret the above histogram. Instead, we can also see the top 20 most common prices

```{r}
#Check the most common prices
most_common_prices <- airbnb_singapore %>% group_by(price) %>% summarize(count = n()) %>% top_n(20, count) %>% arrange(desc(count))
most_common_prices
```

It is found that 50 Singaporean dollars is the most common price with 203 priced at this level. Secondly, 60 Singaporean dollars is commonly used to price Airbnbs as well.

A display of the availability distribution can be shown as well.

```{r}
#Check distributions of availability in the training set
airbnb_singapore %>% ggplot(aes(availability_365)) + geom_histogram(bins = 20, fill = "#009999") +
ggtitle("Availability")
```

The graph shows that while some are only available less than 300 days a year, the far majority of Airbnbs are actually available for rent all 365 days or at least close to all.

The distribution below shows the number of listings in each regions. Each listing have a unique id, thus, by counting the id variable, we assume this will give us the total number of listings, as an Airbnb should only be listed on the website once.

```{r}
#Check distributions of listings in the various regions in training set
airbnb_singapore %>% ggplot(aes(neighbourhood_group)) + geom_bar(bins = 20, fill = "#3399FF") +
ggtitle("Number of listings in regions")
```

From the graph above, we can see that the most popular region with the highest number of Airbnbs is the Central Region. This could indicate this number being very popular tourist destinations but also that there are many houses available, where owners like to rent it out to temporary visitors. 

Next, let's see whether there is a difference in the average price levels across regions.


```{r}
#Check distributions of listings in the various regions in training set

avg_prices <- airbnb_singapore %>% group_by(neighbourhood_group) %>%
summarize(region_avg = mean(price)) %>%
arrange(-region_avg)
avg_prices
#Display them in a histogram
avg_prices %>% ggplot(aes(reorder(neighbourhood_group, region_avg), region_avg, fill = region_avg)) +
geom_bar(stat = "identity") + coord_flip() +
scale_fill_distiller(palette = "YlOrRed") + labs(y = "Region price mean", x = "Region") +
ggtitle("Average Airbnb prices of regions")

```


Show the neighbourhoods with the most listings.
 
```{r}

#First, save the number of listings in each neighbourhood.

top_neighbourhoods <- airbnb_singapore %>% group_by(neighbourhood) %>% summarize(count = n()) %>% top_n(20, count) %>% arrange(desc(count))
top_neighbourhoods

#We can also show them in a bar chart.

top_neighbourhoods %>% ggplot(aes(reorder(neighbourhood, count), count, fill = count)) +
geom_bar(stat = "identity") + coord_flip() +
scale_fill_distiller(palette = "Reds") + labs(y = "Number", x = "Neighbourhood") +
ggtitle("Popular neighbourhoods")

```

From this, we can see that some of the neighbourhoods with most listings are Kallang, Geylang, Novena, Rochor and Bukit Merah.

Let's try to create a map that gives a comprehensive view of the neighbourhoods in the different regions.A distribution of the number of the different room types is also investigated.

```{r}
#Check distributions of listings in the various room types in training set
airbnb_singapore %>% ggplot(aes(room_type)) + geom_bar(bins = 20, fill = "#9999FF") +
ggtitle("Number of listings in regions")
```

The most common room types are entire homes or apartments and private rooms. Hotel rooms and shared rooms are much less common.


We can also check the distribution of the number of reviews.

```{r}
#Check distributions of review count in the training set
airbnb_singapore %>% ggplot(aes(number_of_reviews)) + geom_histogram(bins = 30, fill = "#009999") +
ggtitle("Reviews")
```

This shows that there are only very few who almost have up to a 5000 reviews, but most Airbnbs have below 1000 reviews. To be more specific, it is generally more common for the Airbnb's to have few or none reviews than to have a lot. This could be due to the large number of listings, leading to a great number of options for visitors. Therefore, some may not even get to be visited or reviewed very often or at all.

It's also interesting to look at which words are most commonly used in the Airbnb titles.

First, we need to clean the text. I first saved the columns with the Airbnb names as a text file.
```{r}
#Now, we import the text file with the Airbnb names. We can read the file from GitHub
filePath <- "https://github.com/sarahtomori/cyo-airbnb-singapore/blob/master/singapore-airbnb-names.txt"
text <- readLines(filePath)

#Read the data as a corpus
docs <- Corpus(VectorSource(text))

#Inspect the element of the document
inspect(docs)
```


Now, the text can be transformed, which is done using tm_map in order to replace special characters that cold be found in the text. 

```{r}
#Replace special characters with text
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "!")
```

The next step is to further clean the text.

```{r}
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
```

Afterwards, we can create a matrix that contains the frequence of the words. 
```{r}

#First, create a matrix that contains the frequency of the words. 
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
head(d, 10)

#Create a wordcloud that visualises the most common words used to describe the Airbnbs. 
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35, 
colors=brewer.pal(8, "Dark2"))

```

To see the 10 words are used most frequently, we can use the following code:

```{r}
#Show the 10 most frequently used words.
head(d, 10)

#and plot them
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="#FF9999", main ="Most frequent words",
ylab = "Word frequencies")

```


Another step that is necessary before deciding how to predict Airbnb prices is to check how many of the variables are correlated with each other. If two or more variables are highly correlated, this could dilute the influence the individual variable has on Airbnb prices, as they could initially explain the same thing.Note that only the numerical values are applied in this plot.

```{r}
#Check whether the variables are correlated.

cordata <- airbnb_singapore[, c(1,3,7,8,10,11,12,14,15,16)]
head(cordata)

cordata.cor = cor(cordata, method = c("spearman"))
cordata.rcorr = rcorr(as.matrix(cordata))
cordata.rcorr

#Next, the following code can be used to extract the p-values from the data, using the following code:
cordata.coeff = cordata.rcorr$r
cordata.coeff

cordata.p = cordata.rcorr$P
cordata.p

#The correlation plot can be visualised using the corrplot() function
corrplot(cordata.cor)

```

The correlation matrix above shows the variables that are positively correlated on a red sclae and the negatively correlated variables as blue scale.

#Data analysis

Now that we have reviewed and visualised the data, it is time to decide, which algorithm that does the best job at predicting the price of renting an Airbnb in Singapore.

I used the Spatial Hedonic Price Model (a type of OLS regression) and

The Gradient Boosting method, with the XGBRegressor.

In order to evaluate the best model, RMSE (residual mean squared error) was used to choose the model with the smallest loss.


```{r}
#First, we define the function that calculates RMSE
RMSE <- function(true_price, predicted_prices){
sqrt(mean((true_price - predicted_price)^2, na.rm=T))
}
```

If we were to use the most basic guess, we would simply guess the price to be the average of all prices of listed Airbnb in Singapore.

```{r}
#Calculate average price of Airbnb prices in Singapore

avg_price <- mean(airbnb_singapore$price)
avg_price

```

In this case, the squared loss would be
```{r}
#Check squared loss
mean((avg_price - validation$price)^2)
```

This number is huge and is obviously a very poor estimate of predicting the price of an Airbnb in Singapore. Prices will vary a lot depending on various factors, so the mean is not at all a good estimate. In other words, many prices will vary from the mean and the sum of these differences will eventually add up to a large number as seen above.

First, we can try the most basic linear regression model. This is commonly used to for ML predictions but is a very simple model that works particularly well if there is a high level of linearity.

```{r}
#Create lm model
fit <- lm(price ~ host_id + longitude + latitude + latitude + number_of_reviews + calculated_host_listings_count + neighbourhood_group + neighbourhood + room_type + calculated_host_listings_count + last_review + availability_365, data = airbnb_singapore)
step(fit, direction = "backward", trace = FALSE) #use backward elimination to remove redundant variables

#Let's look at the coefficients
fit$coefficients

#Save neighbourhood and room_type variables as factor in both training and validation sets
validation$neighbourhood <- as.factor(validation$neighbourhood)
airbnb_singapore$neighbourhood <- as.factor(airbnb_singapore$neighbourhood)
validation$room_type <- as.factor(validation$room_type)
airbnb_singapore$room_type <- as.factor(airbnb_singapore$room_type)

#add all levels of 'neighbourhood' in 'validation' dataset to fit$xlevels[["neighbourhood"]] in the fit object
fit$xlevels[["neighbourhood"]] <- union(fit$xlevels[["neighbourhood"]], levels(validation[["neighbourhood"]]))

#add all levels of 'room_type' in 'validation' dataset to fit$xlevels[["room_type"]] in the fit object
fit$xlevels[["room_type"]] <- union(fit$xlevels[["room_type"]], levels(validation[["room_type"]]))

#Use the predict function
y_hat <- predict(fit, validation)
```

Next, we can see whether this model does any better than the basic mean.

```{r}
#Check squared loss
mean((y_hat - validation$price)^2)
RMSE(validation$price, y_hat)
```


Next, the ridge regression model can be used to see if it gives us a better result. The ridge regression is often used in order to deal with the problem of multicollinearity. It uses L2 regularization to penalize the square of magnitude for the coifficients in the regression in order to minimize them.

The formula looks as written below:
LS Obj + L (sum of the square of coefficients)

where L is lambda. If L = 0, then our output will be similar to that of a basic linear regression.
If L is large, then coefficients will move towards zero. The glmnet package in R is used.

Note: I decided the last_review variable as it contains similar information as number of reviews, and it caused a lot of issues with NAs.

```{r}

#Find the earliest review date in the data set
sort.POSIXlt(airbnb_singapore$last_review, origin = "1970-01-01", tz = "GMT")

#Replace missing values in the last_review variable with the date of the first review
airbnb_singapore$last_review <- ifelse(is.na(airbnb_singapore$last_review), as.Date(2013-10-21,  origin = "2013-10-21", tz = "GMT"), airbnb_singapore$last_review)

airbnb_singapore[is.na(airbnb_singapore)] <- 0

#Define the dependent variable
y_var <- airbnb_singapore$price

#Define the independent variables
x_var <- data.matrix(airbnb_singapore[, c("host_id", "longitude", "latitude", "calculated_host_listings_count", "neighbourhood", "last_review", "room_type", "availability_365")])

# Setting the range of lambda values
lambda_seq <- 10^seq(2, -2, by = -.1)

train <- data.frame(x_var, y_var)

model_glmnet <- train(y_var ~ ., data = train,
method = "glmnet",
metric = "RMSE",
na.action = na.replace,
alpha = 0, lambda  = lambda_seq
)

#Check the model
summary(model_glmnet)
model_glmnet

cvfit = cv.glmnet(x_var, y_var)

#Choose the optimal lambda value
# Using cross validation glmnet
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambdas)
# Best lambda value
best_lambda <- model_glmnet$lambda.min
best_lambda

```



```{r}
#Save several models to build a model with several machine learning algorithms

models <- c("glm", "lda", "naive_bayes", "svmLinear",
"gamboost", "gamLoess", "qda",
"knn", "kknn", "loclda", "gam",
"rf", "ranger", "wsrf", "Rborist",
"avNNet", "mlp", "monmlp",
"adaboost", "gbm",
"svmRadial", "svmRadialCost", "svmRadialSigma")
```

